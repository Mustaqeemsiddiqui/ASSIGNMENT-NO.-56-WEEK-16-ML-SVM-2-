{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1563e47-c594-4f42-8a39-cd40b1d1ed3c",
   "metadata": {},
   "source": [
    "**Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e8dee-0a85-4577-8485-1b16c04d8f3f",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "In machine learning, polynomial functions and kernel functions are related concepts, particularly in the context of Support Vector Machines (SVMs) and other kernel-based methods.\n",
    "\n",
    "### Polynomial Functions:\n",
    "A polynomial function is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. For example, a polynomial function of degree \\( d \\) in one variable \\( x \\) can be written as:\n",
    "\\[ f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_d x^d \\]\n",
    "\n",
    "### Kernel Functions:\n",
    "A kernel function is a method used to transform data into a higher-dimensional space, making it easier to classify data that is not linearly separable in the original space. The kernel function essentially computes the dot product of two vectors in a higher-dimensional space without explicitly performing the transformation.\n",
    "\n",
    "### Relationship Between Polynomial Functions and Kernel Functions:\n",
    "The polynomial kernel is a specific type of kernel function that implicitly maps input features into a higher-dimensional space using polynomial functions. It allows algorithms like SVM to create decision boundaries that are polynomial functions of the input features. The polynomial kernel of degree \\( d \\) is defined as:\n",
    "\\[ K(x, y) = (\\gamma x^\\top y + r)^d \\]\n",
    "where \\( x \\) and \\( y \\) are input vectors, \\( \\gamma \\) is a constant that scales the input, \\( r \\) is a constant that shifts the input, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "### Key Points:\n",
    "1. **Implicit Mapping**: The polynomial kernel implicitly maps the input data into a higher-dimensional polynomial feature space without the need to compute the mapping explicitly.\n",
    "2. **Non-linearity**: By using a polynomial kernel, SVMs can create non-linear decision boundaries, which can handle more complex data distributions.\n",
    "3. **Flexibility**: The degree of the polynomial \\( d \\) determines the flexibility of the decision boundary. Higher degrees allow for more complex boundaries but may also increase the risk of overfitting.\n",
    "\n",
    "### Example:\n",
    "Suppose we have two input vectors \\( x = [x_1, x_2] \\) and \\( y = [y_1, y_2] \\). Using a polynomial kernel of degree 2, the kernel function is:\n",
    "\\[ K(x, y) = (\\gamma (x_1 y_1 + x_2 y_2) + r)^2 \\]\n",
    "\n",
    "If we set \\( \\gamma = 1 \\) and \\( r = 0 \\), the kernel function simplifies to:\n",
    "\\[ K(x, y) = (x_1 y_1 + x_2 y_2)^2 \\]\n",
    "Expanding this, we get:\n",
    "\\[ K(x, y) = x_1^2 y_1^2 + 2 x_1 y_1 x_2 y_2 + x_2^2 y_2^2 \\]\n",
    "\n",
    "This shows how the polynomial kernel transforms the input vectors into a higher-dimensional space involving quadratic terms.\n",
    "\n",
    "### Conclusion:\n",
    "Polynomial functions and kernel functions are closely related in the context of machine learning algorithms like SVMs. The polynomial kernel function uses polynomial functions to implicitly map input data into a higher-dimensional space, allowing for more complex decision boundaries and better classification performance on non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d3ac9-5dea-4a85-9099-5edbbc623909",
   "metadata": {},
   "source": [
    "**Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035fc03-bbd6-41f0-874b-479efab7c3c5",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "1. **Data Preparation**:\n",
    "   - We load the Iris dataset and split it into training and testing sets.\n",
    "   - We standardize the features to ensure that each feature has a mean of 0 and a standard deviation of 1. This is important for SVMs to perform well.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - We create an SVM model with a polynomial kernel using `SVC(kernel='poly', degree=3, C=1.0, gamma='scale')`.\n",
    "     - `kernel='poly'` specifies the polynomial kernel.\n",
    "     - `degree=3` specifies the degree of the polynomial kernel.\n",
    "     - `C=1.0` is the regularization parameter.\n",
    "     - `gamma='scale'` is a kernel coefficient.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - We make predictions on the test set.\n",
    "   - We print the classification report and the accuracy score to evaluate the model's performance.\n",
    "\n",
    "We can adjust the `degree`, `C`, and `gamma` parameters to tune the model for better performance on your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30ead40-4baf-4abf-a357-b545deec79d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00f9b9e-9dba-45a1-bb24-96822ea6ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9553d7-75ec-4a00-97c9-afed022bcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34186b77-6bb4-4d3c-a0fd-11f5edb803a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the SVM model with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
    "\n",
    "# Train the model\n",
    "svm_poly.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e4a0227-ec8a-4ecc-9522-2ddf5c5c4511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.87      1.00      0.93        13\n",
      "           2       1.00      0.85      0.92        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ae456-94a5-472c-9b36-f08ba320ebce",
   "metadata": {},
   "source": [
    "**Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94ecb9-4879-411a-8f27-5a3ee9362fb2",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter \\(\\epsilon\\) (epsilon) defines a margin of tolerance where no penalty is given to errors. In other words, it specifies the region within which predictions are considered acceptable or \"close enough\" to the actual values. \n",
    "\n",
    "### Effect of Increasing \\(\\epsilon\\):\n",
    "1. **Wider Margin**:\n",
    "   - Increasing \\(\\epsilon\\) creates a wider margin around the regression function where errors are not penalized.\n",
    "   - This wider margin means that more data points will fall within the \\(\\epsilon\\)-tube, i.e., the region where no loss is assigned.\n",
    "\n",
    "2. **Fewer Support Vectors**:\n",
    "   - As \\(\\epsilon\\) increases, more data points will be considered \"good enough\" without contributing to the penalty term.\n",
    "   - Consequently, fewer data points lie outside the \\(\\epsilon\\)-tube and contribute to the model's error, resulting in fewer support vectors.\n",
    "   - Support vectors are the data points that lie on the edge of the \\(\\epsilon\\)-tube or outside it and are crucial in defining the position and orientation of the regression function.\n",
    "\n",
    "### Intuitive Explanation:\n",
    "- **Small \\(\\epsilon\\)**: With a small \\(\\epsilon\\), the model is more sensitive to errors, and many data points will be outside the \\(\\epsilon\\)-tube. These points become support vectors, and thus the number of support vectors is higher.\n",
    "- **Large \\(\\epsilon\\)**: With a large \\(\\epsilon\\), the model becomes more tolerant to errors within a broader range. Fewer data points lie outside this range, reducing the number of support vectors.\n",
    "\n",
    "### Mathematical Perspective:\n",
    "The optimization problem in SVR aims to find a function \\( f(x) \\) that deviates from the actual target values \\( y \\) by a value no greater than \\(\\epsilon\\) for each training point, while simultaneously being as flat as possible. Increasing \\(\\epsilon\\) reduces the number of constraints (data points outside the \\(\\epsilon\\)-tube), which directly reduces the number of support vectors needed to describe the model.\n",
    "\n",
    "### Example:\n",
    "Imagine a scenario where you are trying to fit an SVR model to some data. If \\(\\epsilon\\) is small, the model will try to fit the data closely, resulting in many support vectors. If \\(\\epsilon\\) is large, the model will ignore small deviations, leading to fewer support vectors.\n",
    "\n",
    "### Conclusion:\n",
    "Increasing the value of \\(\\epsilon\\) in SVR generally decreases the number of support vectors because it allows a wider margin of error around the predicted values, within which errors are not penalized. This makes the model simpler but potentially less sensitive to small variations in the data. \n",
    "\n",
    "In summary:\n",
    "- **Small \\(\\epsilon\\)**: More support vectors, higher sensitivity to data variations.\n",
    "- **Large \\(\\epsilon\\)**: Fewer support vectors, higher tolerance for errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf977dd-bc1d-4597-86df-c0c5af0cd3fc",
   "metadata": {},
   "source": [
    "**Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e8195-a78f-45b2-a5a4-078a2a99736b",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Support Vector Regression (SVR) is a powerful tool for regression tasks, and its performance is influenced by several key parameters: the kernel function, the \\(C\\) parameter, the \\(\\epsilon\\) parameter, and the \\(\\gamma\\) parameter (for certain kernels). Here’s how each of these parameters works and affects the performance of SVR:\n",
    "\n",
    "### 1. Kernel Function:\n",
    "The kernel function defines the type of transformation applied to the input data to map it into a higher-dimensional space where it can be more easily separated or fitted.\n",
    "\n",
    "#### Common Kernel Functions:\n",
    "- **Linear Kernel**: \\( K(x, x') = x \\cdot x' \\)\n",
    "  - **Use**: When the data is linearly separable or nearly so.\n",
    "  - **Example**: Predicting housing prices based on a few linear features.\n",
    "\n",
    "- **Polynomial Kernel**: \\( K(x, x') = (\\gamma x \\cdot x' + r)^d \\)\n",
    "  - **Use**: When there are interactions between features that can be captured by polynomials.\n",
    "  - **Example**: Predicting stock prices where interactions between multiple features are non-linear.\n",
    "\n",
    "- **Radial Basis Function (RBF) Kernel**: \\( K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\)\n",
    "  - **Use**: When the relationship between the target and the features is highly non-linear.\n",
    "  - **Example**: Handwriting recognition, where the data has complex, non-linear patterns.\n",
    "\n",
    "#### Choosing the Kernel:\n",
    "- **Linear**: Start with a linear kernel if the number of features is high relative to the number of samples, as it is less computationally intensive.\n",
    "- **Polynomial/RBF**: Use these when you suspect non-linear relationships in your data. The RBF kernel is especially effective for capturing complex patterns.\n",
    "\n",
    "### 2. \\(C\\) Parameter (Regularization Parameter):\n",
    "The \\(C\\) parameter controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights (model complexity).\n",
    "\n",
    "- **Small \\(C\\)**: Allows more slack (penalizes errors less), leading to a smoother decision function.\n",
    "  - **Example**: When you want to avoid overfitting and can tolerate some errors on the training set.\n",
    "\n",
    "- **Large \\(C\\)**: Penalizes errors more, leading to a tighter fit on the training data.\n",
    "  - **Example**: When you want to achieve a very low training error, even if it risks overfitting.\n",
    "\n",
    "### 3. \\(\\epsilon\\) Parameter:\n",
    "The \\(\\epsilon\\) parameter defines a margin of tolerance where no penalty is given to errors. It creates an \\(\\epsilon\\)-tube around the predicted function within which predictions are considered acceptable.\n",
    "\n",
    "- **Small \\(\\epsilon\\)**: Less tolerance for errors, leading to a more complex model with more support vectors.\n",
    "  - **Example**: When high accuracy is crucial, and you need precise predictions.\n",
    "\n",
    "- **Large \\(\\epsilon\\)**: More tolerance for errors, resulting in a simpler model with fewer support vectors.\n",
    "  - **Example**: When you can tolerate some error and want to simplify the model to generalize better.\n",
    "\n",
    "### 4. \\(\\gamma\\) Parameter (for RBF and Polynomial Kernels):\n",
    "The \\(\\gamma\\) parameter defines how far the influence of a single training example reaches. It determines the curvature of the decision boundary.\n",
    "\n",
    "- **Small \\(\\gamma\\)**: Far-reaching influence, resulting in smoother, less complex decision boundaries.\n",
    "  - **Example**: When you suspect that the data trends are more global and less local.\n",
    "\n",
    "- **Large \\(\\gamma\\)**: Short-reaching influence, resulting in more complex, wiggly decision boundaries.\n",
    "  - **Example**: When you suspect that the data has intricate local variations and patterns.\n",
    "\n",
    "### Example Scenarios:\n",
    "\n",
    "1. **High-Dimensional Data**:\n",
    "   - **Kernel**: Start with a linear kernel.\n",
    "   - **\\(C\\)**: Set a moderate value to balance fitting and regularization.\n",
    "   - **\\(\\epsilon\\)**: Set a small value if precision is important.\n",
    "   - **\\(\\gamma\\)**: Not applicable for a linear kernel.\n",
    "\n",
    "2. **Non-Linear Data with Local Patterns**:\n",
    "   - **Kernel**: Use RBF kernel.\n",
    "   - **\\(C\\)**: Increase to reduce training error.\n",
    "   - **\\(\\epsilon\\)**: Increase to simplify the model.\n",
    "   - **\\(\\gamma\\)**: Increase to capture local patterns.\n",
    "\n",
    "3. **Data with Polynomial Relationships**:\n",
    "   - **Kernel**: Use polynomial kernel.\n",
    "   - **\\(C\\)**: Start with a moderate value.\n",
    "   - **\\(\\epsilon\\)**: Start with a small value.\n",
    "   - **\\(\\gamma\\)**: Adjust based on the degree of polynomial and data complexity.\n",
    "\n",
    "### Conclusion:\n",
    "The performance of SVR can be significantly influenced by the choice of kernel function, \\(C\\) parameter, \\(\\epsilon\\) parameter, and \\(\\gamma\\) parameter. Proper tuning of these parameters is crucial and often requires cross-validation to find the best combination for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e3467-7724-4878-8d4a-866e70a02247",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**\n",
    "    \n",
    " Import the necessary libraries and load the dataset\n",
    "\n",
    " Split the dataset into training and testing sets\n",
    "\n",
    " Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "use the trained classifier to predict the labels of the testing data\n",
    " \n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-score)\n",
    " \n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performance\n",
    "\n",
    "Train the tuned classifier on the entire dataset\n",
    "\n",
    "Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47212497-f0bb-4aa1-b442-bbee9d7030c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a08aeab9-994e-4cb7-9bf6-ec55a1d2cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3354488-f535-4f1f-b693-b70890e96609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d42e88c-367d-407e-a16f-74b01ef636e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the SVC classifier\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c88802b-da3a-4044-b097-95b871fff8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40f4f0b-05b8-4849-b2be-878f5bd7524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the classifier\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed4b9c9-fb59-42a3-b80f-f0a568c64e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV instance on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best Score: {grid_search.best_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b4f0e8b-ccb2-485d-adf9-1a1f3a677d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Fit the classifier on the scaled entire dataset\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "best_svc.fit(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913902e5-7970-4d93-85a2-19fe8c5db05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_svc_classifier.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'best_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3e4f8-7a9e-4f93-8af2-9beddf84a808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
